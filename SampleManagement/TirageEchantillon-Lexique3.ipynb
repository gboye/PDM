{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nomLexiqueOrg=\"/Users/gilles/ownCloud/Python/phonemisation/Lexique380-UTF8.txt\"\n",
    "nomRepertoire=\"/Volumes/gilles/Transfert/Copies-iMac-GB/Python/phonemisation/\"\n",
    "nomLexiqueOrg=nomRepertoire+\"Lexique380-UTF8.txt\"\n",
    "Lexique3=pd.read_csv(nomLexiqueOrg,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'1_ortho', u'2_phon', u'3_lemme', u'4_cgram', u'5_genre', u'6_nombre',\n",
       "       u'7_freqlemfilms2', u'8_freqlemlivres', u'9_freqfilms2',\n",
       "       u'10_freqlivres', u'11_infover', u'12_nbhomogr', u'13_nbhomoph',\n",
       "       u'14_islem', u'15_nblettres', u'16_nbphons', u'17_cvcv', u'18_p_cvcv',\n",
       "       u'19_voisorth', u'20_voisphon', u'21_puorth', u'22_puphon', u'23_syll',\n",
       "       u'24_nbsyll', u'25_cv-cv', u'26_orthrenv', u'27_phonrenv',\n",
       "       u'28_orthosyll', u'29_cgramortho', u'30_deflem', u'31_defobs',\n",
       "       u'32_old20', u'33_pld20', u'34_morphoder', u'35_nbmorph'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexique3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lex3=Lexique3[[\"1_ortho\",\"2_phon\",\"3_lemme\",\"4_cgram\",\"5_genre\",\"6_nombre\",\"7_freqlemfilms2\",\"8_freqlemlivres\",\"9_freqfilms2\",\"10_freqlivres\",\"11_infover\"]].copy()\n",
    "Lexique3=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "unite=1000\n",
    "nbUnites=100\n",
    "nSamples=5\n",
    "echantillon=\"-%dKo\"%nbUnites\n",
    "lex3[\"fcFilms\"]= (lex3[\"9_freqfilms2\"].cumsum()*1000).astype(int)\n",
    "lex3[\"fcTextes\"]= (lex3[\"10_freqlivres\"].cumsum()*1000).astype(int)\n",
    "rangeTopFilms=lex3[\"fcFilms\"].max()\n",
    "rangeTopTextes=lex3[\"fcTextes\"].max()\n",
    "lex3[\"tir1\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tirageIndex():\n",
    "    tirage=[]\n",
    "    for n in range(nbUnites*unite):\n",
    "        tirage.append(random.randrange(rangeTop))\n",
    "    return tirage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def tirage2triage(tirage,lexiqueBase,freqcum=\"freqcum\"):    \n",
    "    triage=sorted(tirage)\n",
    "    freqTop=0\n",
    "    indexMin=0\n",
    "    tirs={}\n",
    "    lexique=lexiqueBase.copy()\n",
    "    lexique[\"tir1\"]=0\n",
    "    for num,tir in enumerate(triage[:]):\n",
    "        if tir > freqTop:\n",
    "            indexMin=lexique[lexique[freqcum]>=tir][0:1].index.astype(int)[0]\n",
    "            freqTop=lexique.ix[indexMin,freqcum]\n",
    "            tirs[indexMin]=0\n",
    "        tirs[indexMin] += 1\n",
    "        if num%5000000==0:\n",
    "            print num,\n",
    "    print\n",
    "    for indexNum in tirs:\n",
    "        lexique.ix[indexNum,'tir1']+=tirs[indexNum]\n",
    "    return lexique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def boucleEchantillons(lexiqueBase,freqcum=\"freqcum\"):\n",
    "    for nSample in range(nSamples):\n",
    "        tirage=tirageIndex()\n",
    "        lexique=tirage2triage(tirage,lexiqueBase,freqcum)\n",
    "        print\n",
    "        print \"nSample\",nSample\n",
    "        allTokens=lexique[lexique[\"tir1\"]!=0][\"tir1\"].sum()\n",
    "        verbTokens=lexique[(lexique[\"tir1\"]!=0) & (lexique[\"4_cgram\"]==\"VER\")][\"tir1\"].sum()\n",
    "        allForms=lexique[lexique[\"tir1\"]!=0][\"tir1\"].count()\n",
    "        verbForms=lexique[(lexique[\"tir1\"]!=0) & (lexique[\"4_cgram\"]==\"VER\")][\"tir1\"].count()\n",
    "        print allTokens,verbTokens, float(verbTokens)/allTokens\n",
    "        print allForms,verbForms,float(verbForms)/allForms\n",
    "#    with open(\"/Volumes/gilles/Transfert/Copies-iMac-GB/2015-Data/Samples/Verbes3/\"+prefix+echantillon+\"-%02d\"%nSample+dateStamp+'-Tirage.pkl', 'wb') as output:\n",
    "#        pickle.dump(lexique, output, pickle.HIGHEST_PROTOCOL)\n",
    "#    lexiqueBase[\"tir1\"]+=lexique[\"tir1\"]\n",
    "#print lexiqueBase[lexiqueBase[\"tir1\"]!=0][\"tir1\"].sum()\n",
    "#print lexiqueBase[lexiqueBase[\"tir1\"]!=0][\"tir1\"].count()\n",
    "#with open(\"/Users/gilles/ownCloud/Recherche/Boye/2015-Data/\"+tiragePrefix+\"-%2d\"%nSamples+\"-Total\"+echantillon+'-Tirage.pkl', 'wb') as output:\n",
    "#    pickle.dump(lexiqueBase, output, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "nSample 0\n",
      "100000 22163 0.22163\n",
      "11057 4200 0.379849868861\n",
      "0\n",
      "\n",
      "nSample 1\n",
      "100000 21885 0.21885\n",
      "11061 4153 0.375463339662\n",
      "0\n",
      "\n",
      "nSample 2\n",
      "100000 21633 0.21633\n",
      "11050 4101 0.371131221719\n",
      "0\n",
      "\n",
      "nSample 3\n",
      "100000 21955 0.21955\n",
      "11049 4192 0.379400850756\n",
      "0\n",
      "\n",
      "nSample 4\n",
      "100000 21901 0.21901\n",
      "10995 4172 0.379445202365\n"
     ]
    }
   ],
   "source": [
    "rangeTop=rangeTopFilms\n",
    "boucleEchantillons(lex3,\"fcFilms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "nSample 0\n",
      "100000 16308 0.16308\n",
      "15651 5805 0.370902817711\n",
      "0\n",
      "\n",
      "nSample 1\n",
      "100000 16566 0.16566\n",
      "15667 5910 0.377226016468\n",
      "0\n",
      "\n",
      "nSample 2\n",
      "100000 16407 0.16407\n",
      "15712 5824 0.37067209776\n",
      "0\n",
      "\n",
      "nSample 3\n",
      "100000 16427 0.16427\n",
      "15842 5837 0.368450953162\n",
      "0\n",
      "\n",
      "nSample 4\n",
      "100000 16644 0.16644\n",
      "15784 5875 0.372212366954\n"
     ]
    }
   ],
   "source": [
    "rangeTop=rangeTopTextes\n",
    "boucleEchantillons(lex3,\"fcTextes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
